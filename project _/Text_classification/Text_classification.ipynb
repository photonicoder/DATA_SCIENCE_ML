{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os,sys\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding  stop_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\n",
    "\"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \n",
    "\"again\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",    \n",
    "\"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"as\", \"at\", \"be\", \"became\", \"because\", \"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"behind\", \"being\", \"beside\", \"besides\", \"between\", \"beyond\", \"both\", \"but\", \"by\",\"can\", \"cannot\", \"cant\", \"could\", \"couldnt\", \"de\", \"describe\", \"do\", \"done\", \"each\", \"eg\", \"either\", \"else\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"find\",\"for\",\"found\", \"four\", \"from\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"i\", \"ie\", \"if\", \"in\", \"indeed\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\",\"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\",\"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"she\", \"should\",\"since\", \"sincere\",\"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"take\",\"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\",\n",
    "\"this\", \"those\", \"though\", \"through\", \"throughout\",\n",
    "\"thru\", \"thus\", \"to\", \"together\", \"too\", \"toward\", \"towards\",\n",
    "\"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "\"very\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "\"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "\"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \n",
    "\"who\", \"whoever\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "\"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X --> list of strings where each string contain one text document.\n",
    "# Y --> corresponding categories\n",
    "X  =[] \n",
    "Y = []\n",
    "for category in os.listdir(\"20_newsgroups1\"):\n",
    "    for document in os.listdir(\"20_newsgroups1/\"+category):\n",
    "        with open(\"20_newsgroups1/\"+category+'/'+document, \"r\") as f:\n",
    "            X.append(f.read())\n",
    "            Y.append(category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting in training and testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(X,Y,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93499\n"
     ]
    }
   ],
   "source": [
    "vocab={}\n",
    "'''\n",
    "    \\W -> Matches any non-alphanumeric character; this is equivalent to the class [^a-zA-Z0-9_].\n",
    "'''\n",
    "for doc in x_train:\n",
    "    doc=doc.lower()\n",
    "    stripped=re.split(r'\\W+',doc)\n",
    "    for word in stripped:\n",
    "        #we will not include stop_words, alpha-numerics, punctuations or irrelevant word of length less than 2 in our dictionary\n",
    "        if word.isalpha() and (word not in stop_words) and len(word)>2:\n",
    "            vocab[word] = vocab.get(word,0)+1\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=sorted(vocab.items(), key=lambda x:x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency of ith word in vocab\n",
    "frequency=[]\n",
    "word_num=[]\n",
    "i=0\n",
    "for w in vocab:\n",
    "    frequency.append(w[1])\n",
    "    word_num.append(i)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot graph between ith word and its frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD8CAYAAACCRVh7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de3hc9X3n8fd37rpavsiykQ02YAyGBGwIOBCIAoRbsoFuoSVpEm9KyrO7NE+y6W4WmqdLkzR5km6TsLQNDRvYQpqGQEIKIWnAAVRCirmDscE3wNjC95ssWbe5/PaP8xt5bCRLMqMzc6TP63n0zDm/+c05X2lG+uh3fufMmHMOERGRocQqXYCIiFQvhYSIiAxLISEiIsNSSIiIyLAUEiIiMiyFhIiIDGtUIWFmG83sFTN7ycye823TzGy5ma33t1N9u5nZrWa2wcxWmtmSku0s8/3Xm9my8fmWRESkXMYykviQc+4M59xZfv1G4FHn3ALgUb8OcDmwwH9dD9wGQagANwPnAGcDNxeDRUREqtO7Odx0JXCXX74LuKqk/W4XWAE0mdls4FJguXNuj3NuL7AcuOxd7F9ERMZZYpT9HPCImTng+86524EW59xWAOfcVjOb6fu2AptLHtvh24ZrP4SZXU8wAiGTyZzZfMxctvc4ZtfFSMfH8J2FqFAoEItV//SO6iwv1Vk+UagRolPnunXrdjnnmsuxrdGGxHnOuS0+CJab2Zoj9LUh2twR2g9tCALodoCFCxe6Hzz0JJ+64xl++p/fz1nzpo2y3HC1t7fT1tZW6TJGpDrLS3WWTxRqhOjUaWZvlWtbo4pE59wWf7sD+DnBnMJ2fxgJf7vDd+8A5pY8fA6w5QjtRxSPBdmSK+g9pkREwjZiSJhZnZk1FJeBS4BVwINA8QylZcADfvlB4NP+LKelQKc/LPUwcImZTfUT1pf4tiNK+KFdXiEhIhK60RxuagF+bmbF/v/snPu1mT0L3Gtm1wGbgGt8/18BVwAbgB7gMwDOuT1m9jXgWd/vq865PSPtXCMJEZHKGTEknHNvAKcP0b4buGiIdgfcMMy27gTuHFOBPiTyhcJYHiYiImVQ9dP0gyOJvEYSIiJhq/qQSMSLIwmFhIhI2Ko/JDQnISJSMVUfEjHTSEJEpFKqPiSKp8BqJCEiEr6qD4m4n5MoKCREREJX9SGhOQkRkcqp+pCI6zoJEZGKqfqQ0EhCRKRyqj4kDo4kFBIiImGr+pDQ2U0iIpVT9SGhkYSISOVUfUgU5ySyeU1ci4iErepDIhYzMskY+3tzlS5FRGTSqfqQAJhWm6KzN1vpMkREJp1IhEQiHqPgNCchIhK2SIREzDRxLSJSCdEIiZiR10hCRCR00QgJM5xCQkQkdJEIibiZDjeJiFRAJEIiFjOUESIi4YtGSJg+T0JEpBIiERLxmOkUWBGRCohESJgZeWWEiEjoIhEScR1uEhGpiEiERMx0uElEpBKiERIxnQIrIlIJkQiJuBkaSIiIhC8SIRGLobflEBGpgGiEhK64FhGpiMiEhN67SUQkfJEIibjeBVZEpCIiERIxMwr6iGsRkdBFJCTQdRIiIhUQiZDQezeJiFTGqEPCzOJm9qKZPeTX55vZ02a23sx+YmYp35726xv8/fNKtnGTb19rZpeOdt+pRIy+rI43iYiEbSwjic8Dr5Wsfwv4rnNuAbAXuM63Xwfsdc6dCHzX98PMFgHXAqcClwHfM7P4aHbcmEnS1ZcdQ6kiIlIOowoJM5sDfAT4gV834ELgp77LXcBVfvlKv46//yLf/0rgHudcv3PuTWADcPZo9t+QSdDVl9NpsCIiIUuMst8twJeABr8+HdjnnMv59Q6g1S+3ApsBnHM5M+v0/VuBFSXbLH3MIDO7HrgeoLm5mfb2dnZuGSBXcDzyWDvpuI36mwtLd3c37e3tlS5jRKqzvFRn+UShRohOneU0YkiY2UeBHc65582srdg8RFc3wn1HeszBBuduB24HWLhwoWtra6Mj8xb3rVvF4ve9n5mNmZFKDl17ezttbW2VLmNEqrO8VGf5RKFGiE6d5TSakcR5wMfM7AogAzQSjCyazCzhRxNzgC2+fwcwF+gwswQwBdhT0l5U+pgjqkkGUxeavBYRCdeIcxLOuZucc3Occ/MIJp4fc879EfA4cLXvtgx4wC8/6Nfx9z/mgsmEB4Fr/dlP84EFwDOjKbImFYREbzY/mu4iIlImo52TGMr/BO4xs78CXgTu8O13AD80sw0EI4hrAZxzq83sXuBVIAfc4Jwb1V/94khCISEiEq4xhYRzrh1o98tvMMTZSc65PuCaYR7/deDrYy0y40OiZyA3Qk8RESmnSFxxPaUmCUBnj66VEBEJUyRCIp0MyhzIa+JaRCRMkQiJuAVnz+qDh0REwhWNkIgpJEREKkEhISIiw4pWSOi9m0REQhWpkChoJCEiEqpohISfuM4pJEREQhWNkIhrTkJEpBKiERJ+JKGPMBURCVc0QiKmw00iIpUQqZDQxLWISLiiERKDV1xXuBARkUkmEiERixlmkC8oJUREwhSJkIBgNKGL6UREwhWZkIjFTIebRERCFpmQSMSMnFJCRCRUkQmJ2lScAwP6+FIRkTBFJiRS8RgDOY0kRETCFJ2QSMTI6nCTiEioIhUSGkmIiIQrWiGhkYSISKiiExKakxARCV1kQqI2lWBf70ClyxARmVQiExLT6lJ09+UqXYaIyKQSmZDQxLWISPiiFRKauBYRCVV0QiIeoz+rkBARCVNkQiKdiNGvkYSISKgiExLFOQmntwsXEQlNZEJiel0KgJ3d/RWuRERk8ohMSNSmEwCalxARCVFkQiKdCErt12mwIiKhiUxIpOJBqbpWQkQkPCOGhJllzOwZM3vZzFab2Vd8+3wze9rM1pvZT8ws5dvTfn2Dv39eybZu8u1rzezSsRSa8iMJXSshIhKe0Ywk+oELnXOnA2cAl5nZUuBbwHedcwuAvcB1vv91wF7n3InAd30/zGwRcC1wKnAZ8D0zi4+20MGQ0EhCRCQ0I4aEC3T71aT/csCFwE99+13AVX75Sr+Ov/8iMzPffo9zrt859yawATh7tIXqcJOISPgSo+nk/+N/HjgR+HvgdWCfc674jnsdQKtfbgU2AzjncmbWCUz37StKNlv6mNJ9XQ9cD9Dc3Ex7ezsAr+8LPt/6uRdfIvf2qMoOTXd392Cd1Ux1lpfqLJ8o1AjRqbOcRvXX1jmXB84wsybg58ApQ3XztzbMfcO1H76v24HbARYuXOja2toAaN7SCSue5ORFp9J22uzRlB2a9vZ2inVWM9VZXqqzfKJQI0SnznIa09lNzrl9QDuwFGgys2LIzAG2+OUOYC6Av38KsKe0fYjHjEinwIqIhG80Zzc1+xEEZlYDXAy8BjwOXO27LQMe8MsP+nX8/Y+54L00HgSu9Wc/zQcWAM+MttBUPJjj1pyEiEh4RnO4aTZwl5+XiAH3OuceMrNXgXvM7K+AF4E7fP87gB+a2QaCEcS1AM651WZ2L/AqkANu8IexRiWd1CmwIiJhGzEknHMrgcVDtL/BEGcnOef6gGuG2dbXga+PvUyd3SQiUgnRueJa10mIiIROISEiIsOKTEgkYkbMoC836mkMERF5lyITEmZGY02S/b25kTuLiEhZRCYkADKJuA43iYiEKFIhkUyYToEVEQlRtEIiHlNIiIiEKFIhkYrHyOpwk4hIaCIVEsl4jKxGEiIioYlYSBjZ/DveOFZERMZJxEIiprObRERCFKmQSCfj9Otwk4hIaCIVEjXJGH0DuuJaRCQskQqJTDKut+UQEQlRtEIiEeftvb2VLkNEZNKIVEj05fLkCo7gg+5ERGS8RSokTmppAPQ51yIiYYlUSNSlgs+57tXktYhIKCIVEjXFkMgqJEREwhCpkMgkg5Do0UhCRCQUkQqJ2lQCgJ4BffCQiEgYIhUSxTkJjSRERMIRqZCoTWskISISpkiFRHEkcaBfIwkRkTBEKiQ0khARCVekQqLeT1x39SkkRETCEK2QyCgkRETCFKmQiMeMulRcISEiEpJIhQRAQyZJd3+20mWIiEwKEQyJBJ29CgkRkTBELiRaGjNs399f6TJERCaFyIVEY02C7n7NSYiIhCFyIVGfTtCtiWsRkVBELiQaM0nNSYiIhCRyITG9Pk1vNs8BHXISERl3I4aEmc01s8fN7DUzW21mn/ft08xsuZmt97dTfbuZ2a1mtsHMVprZkpJtLfP915vZsqMpeEZ9CoBd3Zq8FhEZb6MZSeSAP3POnQIsBW4ws0XAjcCjzrkFwKN+HeByYIH/uh64DYJQAW4GzgHOBm4uBstYNDekAYWEiEgYRgwJ59xW59wLfrkLeA1oBa4E7vLd7gKu8stXAne7wAqgycxmA5cCy51ze5xze4HlwGVjLXhGfRASO7sUEiIi4y0xls5mNg9YDDwNtDjntkIQJGY203drBTaXPKzDtw3Xfvg+ricYgdDc3Ex7e/sh9+/rKwDw5POryOxaO5byx013d/c76qxGqrO8VGf5RKFGiE6d5TTqkDCzeuBnwBecc/vNbNiuQ7S5I7Qf2uDc7cDtAAsXLnRtbW2H38+Xnvw1tc1zaGs7ZbTlj6v29nYOr7Maqc7yUp3lE4UaITp1ltOozm4ysyRBQPzIOXe/b97uDyPhb3f49g5gbsnD5wBbjtA+JmbGrMYM2zr7xvpQEREZo9Gc3WTAHcBrzrnvlNz1IFA8Q2kZ8EBJ+6f9WU5LgU5/WOph4BIzm+onrC/xbWM2qzHD9v0KCRGR8Taaw03nAZ8CXjGzl3zbnwPfBO41s+uATcA1/r5fAVcAG4Ae4DMAzrk9ZvY14Fnf76vOuT1HU3RzY5rVb3cezUNFRGQMRgwJ59yTDD2fAHDREP0dcMMw27oTuHMsBQ6luT7N7gMD73YzIiIygshdcQ0wtTZFV1+ObL5Q6VJERCa0SIbEzMbgWgnNS4iIjK9IhsRx02sBeGt3T4UrERGZ2CIZEsdOC0KiY69CQkRkPEUyJIpvzbGrW5PXIiLjKZIhkUnGacgk9P5NIiLjLJIhAcFpsDu6NHEtIjKeIhsSxzTVsFVvzSEiMq4iGxItjRlWvd1JcO2eiIiMh8iGxEkt9WTzjtVb9le6FBGRCSuyIXHeiTMA2Lj7QIUrERGZuCIbEvNn1AGwaY+ulRARGS+RDYm6dIIZ9Sk26aprEZFxE9mQADhuep0ON4mIjKNoh8S0Wo0kRETGUbRDYnodW/f30ZfNV7oUEZEJKeIhUYtzmrwWERkvkQ6JM+Y2AfDb9bsqXImIyMQU6ZCYN6OOedNrefbNo/qobBERGUGkQwLg1GOm8OpWXXUtIjIeIh8Si45pZNOeHvb3ZStdiojIhBP5kFh6/DQAHlm9vcKViIhMPJEPiSXHTmXe9Fp+/MwmvSOsiEiZRT4kzIxPnHMsz7+1V6fCioiUWeRDAuDCk1sA+Ld1OytciYjIxDIhQuLEmfXMaszwq1e2VroUEZEJZUKEBMBFp8zkpc37yOYLlS5FRGTCmDAhce4JM+jLFljZsa/SpYiITBgTJiTO8afC/suLWypciYjIxDFhQmJGfZrFxzbx6GvbKRR0KqyISDlMmJAA+OPz5rOls48n1ussJxGRcphQIXHJqS3UpuLc/dRblS5FRGRCmFAhkU7EuWBBM4+t2UHHXl1YJyLybk2okAD43EUnAuiaCRGRMhgxJMzsTjPbYWarStqmmdlyM1vvb6f6djOzW81sg5mtNLMlJY9Z5vuvN7Nl4/PtBG8dfvrcJu5/4e3x2oWIyKQxmpHEPwKXHdZ2I/Coc24B8KhfB7gcWOC/rgdugyBUgJuBc4CzgZuLwTIerl7SypptXTz1+u7x2oWIyKQwYkg4554ADv/otyuBu/zyXcBVJe13u8AKoMnMZgOXAsudc3ucc3uB5bwzeMrmmrPm0pBJcN/zm8drFyIik0LiKB/X4pzbCuCc22pmM317K1D6l7nDtw3X/g5mdj3BKITm5mba29uPqsDFM+CXL7/Nh6ftpSZhR7WN0eru7j7qOsOkOstLdZZPFGqE6NRZTkcbEsMZ6q+xO0L7Oxudux24HWDhwoWura3tqAqZesI+fu97v+PhXU3ccu3io9rGaLW3t3O0dYZJdZaX6iyfKNQI0amznI727Kbt/jAS/naHb+8A5pb0mwNsOUL7uDl9bhPXfWA+v1i5VafDiogcpaMNiQeB4hlKy4AHSto/7c9yWgp0+sNSDwOXmNlUP2F9iW8bV8vOnUe+4Pj6L18b712JiExIIx5uMrMfA23ADDPrIDhL6ZvAvWZ2HbAJuMZ3/xVwBbAB6AE+A+Cc22NmXwOe9f2+6pw7fDK87OZMreXkWQ08vnYH+3oGaKpNjfcuRUQmlBFDwjn38WHuumiIvg64YZjt3AncOabqyuDPrziFT9/5DO1rd3LV4iHnykVEZBgT7orrw5134gyaG9Lc/dRGBnL6QCIRkbGY8CERjxlfvuIUXti0j7uf2ljpckREImXChwTAVYtbOfeE6Xz7kXW8vFmfXCciMlqTIiQAvvuHZ5BJxrit/fVKlyIiEhmTJiRaGjN87PRjePjVbfzTCn3ehIjIaJT7iuuqduPlp9Cxt5e/eGAVsxozXLyopdIliYhUtUkzkgCoScX5u08s4YTmev7kh8/xlw+uJq/PwxYRGdakCgkIguKe65fyh2fN5R//fSP/476XyeV1aqyIyFAm1eGmohn1ab75++9lztQa/uaRdezo6ue2Ty6hIZOsdGkiIlVl0o0kSv3phQv4xu+9hxVv7Obj/3cFG3Z0VbokEZGqMqlDAuAT5xzLbZ88k427evjIrU9yW/vrmqcQEfEmfUgAfHhRC7/54gf54EnNfOvXa/jIrb9l3XaNKkREFBLerCkZvv+pM/k/157Bln29/P5t/85v1++sdFkiIhWlkChhZlx5RisPfe58muvTfOqOZ/jv971MZ0+20qWJiFSEQmIIx06v5cHPfYA/OX8+P3uhgw99u51bfrOOnV39lS5NRCRUColh1KcTfPkji/jFn36AU2Y3cMtv1nP+Xz/Gd5avoy+br3R5IiKhUEiM4LTWKfzos0v5zRcv4OJTWrj10fV88H8/zj3PbKKgs6BEZIKblBfTHY0TZzbwd59YwtVn7uBvH9vAjfe/wt8+toEPndzM7FyeDzqHmVW6TBGRslJIjFHbwplcsKCZX6zcwi9XbuXe5zoYyBX4+aYn+OQ5x/IfTj+G6fXpSpcpIlIWComjEIsFZ0FdeUYrPQM5vn3v4/xul/GXv3iVb/xqDR89fTZXnDab80+aQToRr3S5IiJHTSHxLtWmEpw/J8lffPIC1m7r4vYn3uBfV23l/hfepj6doG1hM1ed0cq5J06nNqUft4hEi/5qldHCWQ18+w9O5xv/8TR+u24XD6/exm9e285DK7eSjBtLjp3KH5w1l6UnTKe1qabS5YqIjEghMQ7SiTgXL2rh4kUt5PIFnli/kxVv7OGXK7fyZ/e9DMAJzXWcddw0TprVwOJjm3hv6xQScZ1sJiLVRSExzhLxGBee3MKFJ7fwpUsXsmZbFyve2M2/rdvJw69u4yfPbQagNhVn8bFNnDyrkfe0TuHM46Yyd1pthasXkclOIRGiRDzGaa1TOK11Cp89/3gAtnX28fSbu3nmzT2s7Ojkn1a8RX8u+BCkGfVplhzbxIdOnsnCWQ0cN61WZ06JSKgUEhU2a0pm8EwpgFy+wJptXTy3cQ+rtuzniXU7eeTV7YP950yt4aSWBt7TOoUFLfUcP6Oe+TPqqEnpLCoRKT+FRJUpHW0AFAqOTXt62LCjmzd3HeDFzXtZs7WLx9bsOORxzQ3BqOOklgZmT6mhpTHNcdNrOXZaHamE5jpE5OgoJKpcLGbMm1HHvBl1h7T3DuR5fWc3b+w6wOY9Pby+o5sXNu3lkVe340reLcQMZjVmmDutltpsP6/xOgtm1nPc9FpmTclQn07oSnERGZZCIqJqUvFDRhxFuXyB7V39bNnXy+Y9PWzcdYDNe3t5a/cBnt2eo71jzaHbScaZXp9iRn2aGfVpWpsyNDcEy9Pr0zTVJpnp1+vSermITDb6rZ9gEvEYrU01tDbV8L550w65r729ncXnnMeGHV107O1l+/4+tnX2s+dAP7sPDLBpzwGefmM3Xf25Ibddk4wza0oQIo2ZJFNrk9RnEkyvSzG9Ps0xTTVMrU0yrS5FY02SBo1SRCJPITHJTKlJcuZx0zjzuOH79GXz7D4wwM6ufvb2DLC7e4AdXX3s7h5gW2cfu7r76djbw+otWfb3ZjkwMPRbpydiRk0qTlNtkvp0kqaaIFQaMgmm+BBprEmSScaZWpsik4xRk4yzsTPP+u1dZJJxf3+MVDymwBGpAIWEvEMmGR8cjYxGfy7Pzq5+tuzro7M3y+7ufvb3ZdnXk6VnIM/engEO9Ae3m/f0sL83S1d/jgP9OYZ9t/WnnjhkNRWPUZeOU+ODI52IkUrEqEsnaMgkqU/HacwkSSfjNGYS1KUTpBMxptQkqUnFqUslBpen1CRJxWPEYgodkZEoJORdSyfizJlay5ypY7v4L5cv0JPN09Ofp7M3S38uT3d/jqeff4kFJy/iQH+Orr4c/bmCH7Hk6B0osL8vy0CuwECuwK7uft7a3UNXX3aw72jFDJLxGA2ZIHTSiRj1mQQ1yXgQQKkgbFIJIxU/9L6GTJI3t+ToXrmFZDxGYyZJKhGMeDLJGLXpBMmYkYjHSMSNTCKus8wkkhQSUjGJeIxG/wd21pTMYPvA5gRt7z3mqLZZKDj292Xpyxboywajl75sge7+HN39WQ74QMrlHdl8gYF8ga6+LP0+dLr6cvRm83T15dja2UfvQH6wX2dv9pAzxwBY+eLov9+YkYgbyVgQHPWZBMl4bHA97UdG8ZiRiJm/jZH2h+GS8VjQFjfSiTi1qXhJPyMeC0ZbxX5xM+JxY/WuHOnXd1ObihOPGTELthGzg499536DWx3iE4WETCixmNFUmxpcn0fdEXqPjXOObN4xkA9GNr/93VMsOet9wUinL0s278jmCn7EkydXcOTyBXIFR89Anr5s0JbNF8jmg0Aa7JN39OXyHOjPkS8E+8kXHLlCgd6BPAP5wmBbMbTeEVhH8tyKMX+/ZlCXSmDGYOjEiuHjQ6QYPMXwOdiPwVFXMWyKjzODuAWjrNpUnJhBR8cAK3rXEI9BzIL+Md8v5vcT821mxf3h+wXLweOCmov7iZXcX9xmrOSx6cTB+a5D+vhtHbIOdPY7uvqyJOOxIbc/ESkkREbJzIJDT4kY9ekELXUxFrQ0VKSWnA+NXKEwGCq5QoED/fnBcCkUIFco8NzzL3Dqe06nNxvcly848i64LThHNufoGQgCa/C+vKM/Vxh8jHPFxwSjtbwLwqpnID+4XtxevuAoFGBndz+5/MH2guPgciEI275swdeQxza96fu54eeqqsHjjwx711ABEzejJpXwQXIwcMwOBtmh7Qcfm4gFhy9LQ8zw4RQD453bKPdUm0JCJIKCuQ6Akd+OpfONOOeeOGPca3o32tvbaWtrG1x3zuEcFJwjV3D0DuQHw8P522KguMFlyBd8cPm2wb6FoR9bHPE5ivcFAec4+Phg+8H62nXrmHPc8eR9+6HbAw5bL44+e7N5oLjt4n3++xzcl78t+f77c8Go853fZ7DdwuDjD/2ZlZNCQkSqzuB/2RiJeHDGXTVo73uTtg+eUOkyRmSfL+O2XJlTp5zMrAtYW+k6RmEGsKvSRYyC6iwv1Vk+UagRolPnQudcWY6FVvtIYq1z7qxKFzESM3tOdZaP6iyvKNQZhRohWnWWa1s6cVtERIalkBARkWFVe0jcXukCRkl1lpfqLK8o1BmFGmES1lnVE9ciIlJZ1T6SEBGRClJIiIjIsKo2JMzsMjNba2YbzOzGCuz/TjPbYWarStqmmdlyM1vvb6f6djOzW32tK81sScljlvn+681sWZlrnGtmj5vZa2a22iy4hKYK68yY2TNm9rKv8yu+fb6ZPe33+RMzS/n2tF/f4O+fV7Ktm3z7WjO7tJx1luwjbmYvmtlD1VqnmW00s1fM7KXi6Y7V9rz77TeZ2U/NbI1/nb6/2uo0s4X+51j82m9mX6jCOv+b//1ZZWY/9r9X4//adIOXnlfPF8F7DbwOHA+kgJeBRSHXcAGwBFhV0vbXwI1++UbgW375CuBfAQOWAk/79mnAG/52ql+eWsYaZwNL/HIDsA5YVIV1GlDvl5PA037/9wLX+vZ/AP6LX/6vwD/45WuBn/jlRf61kAbm+9dIfBye+y8C/ww85Nerrk5gIzDjsLaqet79Pu4CPuuXU0BTNdZZUm8c2AYcV011Aq3Am0BNyWvyP4Xx2iz7D7lMP5D3Aw+XrN8E3FSBOuZxaEisBWb75dkEF/sBfB/4+OH9gI8D3y9pP6TfONT7APDhaq4TqAVeAM4huHI1cfhzDjwMvN8vJ3w/O/x1UNqvjPXNAR4FLgQe8vutxjo38s6QqKrnHWgk+MNm1VznYbVdAvyu2uokCInNBAGU8K/NS8N4bVbr4abiD6Sow7dVWotzbiuAv53p24erN7Tvww8nFxP8l151dfpDOC8BO4DlBP/B7HPOFT9Qu3Sfg/X4+zuB6WHUCdwCfAkofnrR9Cqt0wGPmNnzZna9b6u25/14YCfw//zhux+YWV0V1lnqWuDHfrlq6nTOvQ38DbAJ2ErwWnueEF6b1RoSQ73ZbTWfqztcvaF8H2ZWD/wM+IJzbv+Rug5Tz7jX6ZzLO+fOIPhP/WzglCPssyJ1mtlHgR3OuedLm4+wz0o+7+c555YAlwM3mNkFR+hbqToTBIdsb3POLQYOEBy2GU6lf49SwMeA+0bqOkw941annw+5kuAQ0TFAHcFzP9z+ylZjtYZEBzC3ZH0OsKVCtZTabmazAfztDt8+XL3j/n2YWZIgIH7knLu/Wusscs7tA9oJjuU2mVnx/cNK9zlYj79/CrAnhDrPAz5mZhuBewgOOd1ShXXinNvib3cAPycI3mp73juADufc0379pwShUW11Fl0OvOCc2+7Xq3gnL7wAAAGhSURBVKnOi4E3nXM7nXNZ4H7gXEJ4bVZrSDwLLPAz9ymCIeCDFa4JghqKZywsI5gDKLZ/2p/1sBTo9MPTh4FLzGyq/0/gEt9WFmZmwB3Aa86571Rxnc1m1uSXawhe8K8BjwNXD1Nnsf6rgcdccAD1QeBaf+bGfGAB8Ey56nTO3eScm+Ocm0fwmnvMOfdH1VanmdWZWUNxmeD5WkWVPe/OuW3AZjNb6JsuAl6ttjpLfJyDh5qK9VRLnZuApWZW63/viz/L8X9tjsfkT5kmaq4gOFvndeDLFdj/jwmO/WUJ0vc6gmN6jwLr/e0039eAv/e1vgKcVbKdPwY2+K/PlLnGDxAMFVcCL/mvK6qwzvcCL/o6VwH/y7cf71+gGwiG+GnfnvHrG/z9x5ds68u+/rXA5eP4/Ldx8OymqqrT1/Oy/1pd/P2otufdb/8M4Dn/3P8LwVk/1VhnLbAbmFLSVlV1Al8B1vjfoR8SnKE07q9NvS2HiIgMq1oPN4mISBVQSIiIyLAUEiIiMiyFhIiIDEshISIiw1JIiIjIsBQSIiIyrP8Pl14sM4CZVeQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(word_num, frequency)\n",
    "plt.grid()\n",
    "plt.axis([0,8000, 1, 5000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest frequency of a word in vacabulary is : 123562\n",
      "Lowest frequency of a word in vacabulary is : 1\n",
      "Length of vocab is: 93499\n"
     ]
    }
   ],
   "source": [
    "print(\"Highest frequency of a word in vacabulary is :\",vocab[0][1])\n",
    "print(\"Lowest frequency of a word in vacabulary is :\",vocab[len(vocab)-1][1])\n",
    "print(\"Length of vocab is:\",len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### We consisder top 2000 words only bcz as we increase words, frequency decreases as we see in above plot. Even word at 2000 index having frequecy less than 500. So in training matrix which we create , column corresponding to this word will contain mostly zeros. So no need to add furher words bcz they will not affect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### new vocab --> Consisder only top ind words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    create dictionary bcz it will be easy to check word present or not. \n",
    "    If we use list, then everytime we need to traverse to find paticular word present in list or not\n",
    "''' \n",
    "Vocab = {}\n",
    "for i in range(ind):\n",
    "    w=vocab[i][0]\n",
    "    f=vocab[i][1]\n",
    "    Vocab[w]=f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create features dictionary - we use words as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "for i in range(ind):\n",
    "    w_i = vocab[i][0]\n",
    "    features[w_i] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we create X_train, Y_train, X_test and Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros((len(x_train),ind), dtype='uint8')\n",
    "Y_train = np.empty(len(y_train), dtype='<U30') # max size of string is 30\n",
    "\n",
    "X_test = np.zeros((len(x_test),ind), dtype='uint8')\n",
    "Y_test = np.empty(len(y_test), dtype='<U30') # max size of string is 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train and X_test\n",
    "total_training_doc = len(x_train)\n",
    "for i in range(total_training_doc):\n",
    "    stripped_words = re.split(r'\\W+',x_train[i]) ## using regular expressions\n",
    "    for word in stripped_words:\n",
    "        # check if word is present in Vocab and word is also used as a feature\n",
    "        if Vocab.get(word.lower(), None) is not None and features.get(word.lower(),None) is not None:\n",
    "            col = features[word.lower()]\n",
    "            X_train[i][col] += 1 # Increase frequency corresponding to word in document i\n",
    "            \n",
    "    # store corresponding newsgroup (label)\n",
    "    Y_train[i] = y_train[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test and Y_test\n",
    "total_testing_doc = len(x_test)\n",
    "for i in range(total_testing_doc):\n",
    "    stripped_words = re.split(r'\\W+',x_test[i]) ## using regular expressions\n",
    "    for word in stripped_words:\n",
    "        # check if word is present in Vocab and word is also used as a feature\n",
    "        if Vocab.get(word.lower(), None) is not None and features.get(word.lower(),None) is not None:\n",
    "            col = features[word.lower()]\n",
    "            X_test[i][col] += 1 # Increase frequency corresponding to word in document i\n",
    "            \n",
    "    # store corresponding newsgroup (label)\n",
    "    Y_test[i] = y_test[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions used for creating our own Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit fnction returns a dictionary\n",
    "def fit(X_train, Y_train):\n",
    "    count = {}\n",
    "    vocab_size = 0  # total number of words present in Vocab\n",
    "    count[\"total_data\"] = len(Y_train) # total number of documents\n",
    "    class_values = set(Y_train)\n",
    "    for current_class in class_values:\n",
    "        count[current_class] = {}\n",
    "        current_class_rows = (Y_train == current_class)\n",
    "        \n",
    "        # Get X_train corresponding to current_class\n",
    "        X_train_current = X_train[current_class_rows]\n",
    "        Y_train_current = Y_train[current_class_rows]\n",
    "        \n",
    "        # total no. of documents of current_class\n",
    "        count[current_class][\"current_class_count\"] = len(Y_train_current)\n",
    "        \n",
    "        # Iterate over all words and store its frequency\n",
    "        num_features = X_train.shape[1]\n",
    "        total_current_class_words = 0\n",
    "        for j in range(1, num_features + 1):\n",
    "            # store frequency of j th word(feature) corresponding to current_class \n",
    "            count[current_class][j] = X_train_current[:, j-1].sum()\n",
    "            total_current_class_words += X_train_current[:, j-1].sum()\n",
    "            \n",
    "        # store total words of a current_class also\n",
    "        count[current_class][\"total_count\"] = total_current_class_words\n",
    "        vocab_size += total_current_class_words\n",
    "    count[\"vocab_size\"] = vocab_size\n",
    "    return count # returns dictionary created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to calculate log( P(X=x | y=current_class) ) + log( P(y=current_class) )\n",
    "        \n",
    "Assuming independent events:\n",
    "P(X=x | y=current_class) = P(w1|y=current_class) * P(w2|y=current_class) *\n",
    "                                    P(w3|y=current_class) * ....\n",
    "        \n",
    "        \n",
    "Therefore, log( P(X=x | y=current_class) ) = log( P(w1|y=current_class) )\n",
    "                                                                +\n",
    "                                             log( P(w2|y=current_class) )\n",
    "                                                                +\n",
    "                                             log( P(w3|y=current_class) )\n",
    "                                                                +\n",
    "                                                                .\n",
    "                                                                .\n",
    "                                                                ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(dictionary, x, current_class):\n",
    "    #  log( P(y=current_class) )\n",
    "    output = np.log(dictionary[current_class]['current_class_count']) - np.log(dictionary[\"total_data\"]) \n",
    "    \n",
    "    num_features = len(dictionary[current_class].keys()) - 2  # -2 bcz 2 keys are total_count and current_class_count\n",
    "    for j in range(1,num_features+1):\n",
    "        xj = x[j-1]\n",
    "        if xj == 0: # word not in test data document\n",
    "            continue\n",
    "            \n",
    "        '''\n",
    "            word is present as a feature. So add its corresponding logarithmic probability\n",
    "            Also use laplace correction i.e\n",
    "            log( P(w1|y=current_class) ) = log( count of all w1 corresponding \n",
    "                                                to current_class + 1)\n",
    "                                                      -\n",
    "                                            log( count of all words of current_class\n",
    "                                                 + size of vocabulary)\n",
    "        '''  \n",
    "        count_jth_word_in_current_class = dictionary[current_class][j] + 1  # +1 bcz of laplace correction\n",
    "        count_current_class = dictionary[current_class][\"total_count\"] + dictionary[\"vocab_size\"]\n",
    "        count_xj_probability = np.log(count_jth_word_in_current_class) - np.log(count_current_class)\n",
    "        output = output + count_xj_probability\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSinglePoint(dictionary, x):\n",
    "    classes = dictionary.keys()\n",
    "    best_p = -1000   # Initially\n",
    "    best_class = -1\n",
    "    first_run = True  # Bcz best_p should be changed for the first time\n",
    "    for current_class in classes:\n",
    "        if current_class == \"vocab_size\" or current_class==\"total_data\":\n",
    "            continue\n",
    "            \n",
    "        # Find logarithmic probability of current_class\n",
    "        p_current_class = probability(dictionary, x, current_class)\n",
    "        if (first_run or p_current_class > best_p):\n",
    "            best_p = p_current_class\n",
    "            best_class = current_class\n",
    "        first_run = False  # bcz we done for 1st time\n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dictionary, X_test):\n",
    "    y_pred = []\n",
    "    for x in X_test:\n",
    "        x_class = predictSinglePoint(dictionary, x)\n",
    "        y_pred.append(x_class)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find predictions using our own Multinomial classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = predict(dictionary, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[176   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   1\n",
      "   10  45]\n",
      " [  0 184   2   7   0  34   0   0   0   0   0  17   0   0   1   0   0   0\n",
      "    8   0]\n",
      " [  0  23 113   9   0  75   0   0   0   0   0  18   0   0   1   0   1   0\n",
      "    9   0]\n",
      " [  0   8   1 204   3   8   2   0   0   0   0  10   0   0   0   0   0   0\n",
      "    4   0]\n",
      " [  0   6   1  42 152  15   2   0   0   0   0  10   1   0   0   0   1   1\n",
      "    5   0]\n",
      " [  0   8   4   3   0 218   1   0   0   0   0   4   0   0   0   0   0   0\n",
      "    2   0]\n",
      " [  0   1   1  18   0   1 180   3   0   0   0   6   7   0   2   0   1   0\n",
      "   41   0]\n",
      " [  0   1   0   0   0   1   5 190   0   0   1   2   3   1   1   0  25   2\n",
      "   37   0]\n",
      " [  4   1   0   0   0   0   1  11 166   0   0   8   0   1   0   0  24   2\n",
      "   66   0]\n",
      " [  0   0   0   0   0   0   0   0   0 190  26   0   0   0   1   0   1   1\n",
      "   29   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 221   0   0   0   0   0   1   0\n",
      "    9   0]\n",
      " [  0   1   0   0   0   0   0   0   0   0   0 229   0   0   0   0   0   0\n",
      "    3   0]\n",
      " [  0   6   0   3   0   0   0   0   0   1   0  55 137   2   7   0   2   2\n",
      "   28   1]\n",
      " [  0   4   0   0   0   1   0   0   0   0   0   9   0 172   2   0   1   3\n",
      "   59   5]\n",
      " [  0   1   0   0   0   0   0   0   0   0   0   6   0   0 205   0   2   1\n",
      "   31   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 251   0   0\n",
      "    1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   2   0   0   0   0 156   2\n",
      "   86   3]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 265\n",
      "   16   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   7  14\n",
      "  232   5]\n",
      " [ 36   0   0   0   0   0   0   0   0   0   0   2   0   0   0   3   7   5\n",
      "   50 133]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.81      0.76      0.78       233\n",
      "           comp.graphics       0.75      0.73      0.74       253\n",
      " comp.os.ms-windows.misc       0.93      0.45      0.61       249\n",
      "comp.sys.ibm.pc.hardware       0.71      0.85      0.78       240\n",
      "   comp.sys.mac.hardware       0.98      0.64      0.78       236\n",
      "          comp.windows.x       0.62      0.91      0.73       240\n",
      "            misc.forsale       0.94      0.69      0.80       261\n",
      "               rec.autos       0.93      0.71      0.80       269\n",
      "         rec.motorcycles       1.00      0.58      0.74       284\n",
      "      rec.sport.baseball       0.99      0.77      0.87       248\n",
      "        rec.sport.hockey       0.89      0.96      0.92       231\n",
      "               sci.crypt       0.60      0.98      0.75       233\n",
      "         sci.electronics       0.93      0.56      0.70       244\n",
      "                 sci.med       0.98      0.67      0.80       256\n",
      "               sci.space       0.93      0.83      0.88       246\n",
      "  soc.religion.christian       0.99      1.00      0.99       252\n",
      "      talk.politics.guns       0.68      0.63      0.65       249\n",
      "   talk.politics.mideast       0.89      0.94      0.91       281\n",
      "      talk.politics.misc       0.32      0.90      0.47       259\n",
      "      talk.religion.misc       0.69      0.56      0.62       236\n",
      "\n",
      "                accuracy                           0.75      5000\n",
      "               macro avg       0.83      0.76      0.77      5000\n",
      "            weighted avg       0.83      0.75      0.77      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using inbuilt Multinomial Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[195   0   0   0   0   0   0   4   1   0   0   0   0   1   0   1   0   0\n",
      "    0  31]\n",
      " [  0 199  16  21   3   8   2   2   0   1   0   0   0   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0   6 210  13   4   9   3   1   0   0   0   0   2   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   3 196  32   0   5   0   1   0   0   0   2   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   2   1  23 204   0   5   0   0   0   0   0   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0  15  30  13   5 165   3   0   2   1   0   1   2   1   2   0   0   0\n",
      "    0   0]\n",
      " [  0   0   1   7   3   0 236   5   1   0   1   0   5   0   1   0   1   0\n",
      "    0   0]\n",
      " [  0   0   0   1   1   0  11 247   3   1   1   0   4   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   2   4 277   1   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   2   4   2 227  12   0   0   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   2   7 221   0   0   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   3   0   0   0   2   1   0   0   0   0 224   2   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0   2   0  10   3   0   2   7   2   1   0   0 214   2   1   0   0   0\n",
      "    0   0]\n",
      " [  0   3   0   2   6   1   1   3   9   3   0   1   3 219   4   0   0   1\n",
      "    0   0]\n",
      " [  0   1   0   1   0   0   1   2   2   2   1   0   4   3 227   0   1   0\n",
      "    1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 252   0   0\n",
      "    0   0]\n",
      " [  0   0   0   1   0   0   0   0   3   0   0   3   0   0   0   0 224   0\n",
      "   13   5]\n",
      " [  0   0   0   1   1   0   3   3   1   2   1   0   0   1   1   0   6 238\n",
      "   21   2]\n",
      " [  0   0   0   0   0   0   1   0   0   1   0   4   0   3   2   2  33  15\n",
      "  177  21]\n",
      " [ 45   1   0   0   0   0   1   0   0   0   1   0   1   1   0  10  11   2\n",
      "   21 142]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.81      0.84      0.82       233\n",
      "           comp.graphics       0.85      0.79      0.82       253\n",
      " comp.os.ms-windows.misc       0.80      0.84      0.82       249\n",
      "comp.sys.ibm.pc.hardware       0.68      0.82      0.74       240\n",
      "   comp.sys.mac.hardware       0.78      0.86      0.82       236\n",
      "          comp.windows.x       0.89      0.69      0.78       240\n",
      "            misc.forsale       0.85      0.90      0.87       261\n",
      "               rec.autos       0.88      0.92      0.90       269\n",
      "         rec.motorcycles       0.91      0.98      0.94       284\n",
      "      rec.sport.baseball       0.92      0.92      0.92       248\n",
      "        rec.sport.hockey       0.93      0.96      0.94       231\n",
      "               sci.crypt       0.96      0.96      0.96       233\n",
      "         sci.electronics       0.89      0.88      0.88       244\n",
      "                 sci.med       0.94      0.86      0.89       256\n",
      "               sci.space       0.95      0.92      0.93       246\n",
      "  soc.religion.christian       0.95      1.00      0.97       252\n",
      "      talk.politics.guns       0.81      0.90      0.85       249\n",
      "   talk.politics.mideast       0.93      0.85      0.89       281\n",
      "      talk.politics.misc       0.76      0.68      0.72       259\n",
      "      talk.religion.misc       0.71      0.60      0.65       236\n",
      "\n",
      "                accuracy                           0.86      5000\n",
      "               macro avg       0.86      0.86      0.86      5000\n",
      "            weighted avg       0.86      0.86      0.86      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between inbuilt and our own built classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From above results we see that inbuilt Multinomial naive bayes has acuuracy of 86%. \n",
    "#### Where as accuracy achieved by using our own Multinomial classifier is 75%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
